# -*- coding: utf-8 -*-
"""CNN-LSTM with Attention, Auxiliary Features, and SHAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvtNZwVzOFHmfUoCjra4FtrPOUBbT79R
"""

# Google Colab Notebook for Advanced Occupancy Prediction
# (CNN-LSTM with Attention, Auxiliary Features, and SHAP)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Dense, LSTM, Conv1D, MaxPooling1D,
                                     TimeDistributed, Flatten, concatenate,
                                     Attention, Reshape)
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from datetime import datetime, time

# New Imports for Evaluation Metrics
from sklearn.metrics import r2_score, mean_squared_error
from math import sqrt
# Install SHAP library
# !pip install shap

import shap

# --- Helper Function for Metrics Calculation ---
def calculate_metrics(y_true, y_pred):
    """Calculates R-squared, MSE, and RMSE."""
    # R-squared (Coefficient of Determination)
    r2 = r2_score(y_true, y_pred)

    # Mean Squared Error (MSE)
    mse = mean_squared_error(y_true, y_pred)

    # Root Mean Squared Error (RMSE)
    rmse = sqrt(mse)

    return r2, mse, rmse

def create_schedule_features(df_index):
    """
    Create comprehensive schedule-based features for the university library
    """
    features = pd.DataFrame(index=df_index)

    # Basic time features
    features['hour'] = df_index.hour
    features['day_of_week'] = df_index.dayofweek # Monday=0, Sunday=6
    features['date'] = df_index.date

    # Academic schedule features
    features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
    features['is_sunday'] = (features['day_of_week'] == 6).astype(int)

    # Library operating hours
    # Weekdays: 7:30 AM - 8:00 PM (19:30 - 20:00 in 24hr format)
    # Saturday: 7:30 AM - 12:00 PM (07:30 - 12:00)
    # Sunday: Closed
    features['library_open'] = 0

    # Weekday hours (Mon-Fri): 7:30 AM - 8:00 PM
    weekday_mask = (features['day_of_week'] < 5)
    weekday_hours_mask = (features['hour'] >= 7) & (features['hour'] < 20)
    # Include 7:30 AM slot (hour 7 covers 7:00-7:59, so 7:30 is included)
    features.loc[weekday_mask & weekday_hours_mask, 'library_open'] = 1

    # Saturday hours: 7:30 AM - 12:00 PM
    saturday_mask = (features['day_of_week'] == 5)
    saturday_hours_mask = (features['hour'] >= 7) & (features['hour'] < 12)
    features.loc[saturday_mask & saturday_hours_mask, 'library_open'] = 1

    # Class schedule features (7:30 AM - 9:30 PM on weekdays)
    features['class_hours'] = 0
    class_hours_mask = (features['hour'] >= 7) & (features['hour'] < 22)  # 7:30 AM - 9:30 PM
    features.loc[weekday_mask & class_hours_mask, 'class_hours'] = 1

    # Activity period (Mon, Wed 3-6 PM) - no classes but students may use library
    features['activity_period'] = 0
    activity_days = (features['day_of_week'] == 0) | (features['day_of_week'] == 2)  # Mon=0, Wed=2
    activity_hours = (features['hour'] >= 15) & (features['hour'] < 18)  # 3-6 PM
    features.loc[activity_days & activity_hours, 'activity_period'] = 1

    # Peak academic hours (common class times)
    # Morning peak: 8-11 AM, Afternoon peak: 1-4 PM, Evening: 6-8 PM
    features['morning_peak'] = ((features['hour'] >= 8) & (features['hour'] < 11) & weekday_mask).astype(int)
    features['afternoon_peak'] = ((features['hour'] >= 13) & (features['hour'] < 16) & weekday_mask).astype(int)
    features['evening_peak'] = ((features['hour'] >= 18) & (features['hour'] < 20) & weekday_mask).astype(int)

    # Holiday flags
    features['is_holiday'] = 0

    # Specific holidays for 2024 (adjust year as needed)
    holidays_2024 = [
        '2024-08-15',  # Kadayawan holiday
        '2024-08-21',  # Ninoy Aquino Day (Thursday)
        '2024-08-25',  # National Heroes' Day (Monday)
    ]

    for holiday in holidays_2024:
        holiday_date = pd.to_datetime(holiday).date()
        features.loc[features['date'] == holiday_date, 'is_holiday'] = 1

    # Preliminary week (Aug 18-23, 2024) - no regular classes
    prelim_start = pd.to_datetime('2024-08-18').date()
    prelim_end = pd.to_datetime('2024-08-23').date()
    features['is_preliminary'] = 0
    prelim_mask = (features['date'] >= prelim_start) & (features['date'] <= prelim_end)
    features.loc[prelim_mask, 'is_preliminary'] = 1

    # During preliminary week, override class_hours since there are no regular classes
    features.loc[prelim_mask, 'class_hours'] = 0

    # Study intensity indicator (combination of factors that increase library usage)
    features['study_intensity'] = (
        features['library_open'] *
        (features['class_hours'] + features['activity_period'] +
         (1 - features['is_holiday']) + (1 - features['is_preliminary']))
    ).clip(0, 1)

    # Time of day categories for cyclic encoding
    features['time_category'] = 'other'
    features.loc[(features['hour'] >= 7) & (features['hour'] < 12), 'time_category'] = 'morning'
    features.loc[(features['hour'] >= 12) & (features['hour'] < 17), 'time_category'] = 'afternoon'
    features.loc[(features['hour'] >= 17) & (features['hour'] < 20), 'time_category'] = 'evening'

    return features

# --- 1. Load Data and Engineer Features ---
file_names = ['Miguel_Pro_cleaned.csv', 'Gisbert_2nd_Floor_cleaned.csv', 'American_Corner_cleaned.csv',
              'Gisbert_4th_Floor_cleaned.csv', 'Gisbert_5th_Floor_cleaned.csv', 'Gisbert_3rd_Floor_cleaned.csv']

# DataFrame to store results for all files
results_df = pd.DataFrame(columns=['Location', 'R2_Score', 'MSE', 'RMSE'])


for file_name in file_names:
    try:
        df = pd.read_csv(file_name)
    except FileNotFoundError:
        print(f"Error: File '{file_name}' not found. Please upload it.")
        continue # Skip to the next file if not found

    df['Start_dt'] = pd.to_datetime(df['Start_dt'])
    df.set_index('Start_dt', inplace=True)
    # NOTE: Assuming the column name is 'Client_MAC' as per the provided script
    # occupancy = df['Client_MAC'].resample('H').nunique().to_frame(name='occupancy')

    # Check available columns in the dataframe
    print(f"Columns available in {file_name}: {df.columns.tolist()}")

    # Assuming 'Client MAC' is the correct column based on the variable inspection
    if 'Client MAC' in df.columns:
        occupancy = df['Client MAC'].resample('H').nunique().to_frame(name='occupancy')
    else:
        print(f"Error: 'Client MAC' column not found in {file_name}. Skipping this file.")
        continue


    # **Enhanced Feature Engineering with School Schedule**
    schedule_features = create_schedule_features(occupancy.index)

    # Combine with occupancy data
    occupancy = occupancy.join(schedule_features)

    # Add cyclic encoding for hour (captures the circular nature of time)
    occupancy['hour_sin'] = np.sin(2 * np.pi * occupancy['hour'] / 24)
    occupancy['hour_cos'] = np.cos(2 * np.pi * occupancy['hour'] / 24)

    # Add cyclic encoding for day of week
    occupancy['dow_sin'] = np.sin(2 * np.pi * occupancy['day_of_week'] / 7)
    occupancy['dow_cos'] = np.cos(2 * np.pi * occupancy['day_of_week'] / 7)

    print(f"\nProcessing data for: {file_name}")
    print("Data with Enhanced Schedule Features:")
    print(occupancy.head(10))
    print(f"Feature columns: {list(occupancy.columns)}")
    print(f"Data shape: {occupancy.shape}")

    # --- 2. Scale Data and Create Sequences ---
    # Scale the main time series feature (occupancy)
    occ_scaler = MinMaxScaler(feature_range=(0, 1))
    occupancy['occupancy_scaled'] = occ_scaler.fit_transform(occupancy[['occupancy']])

    # Select features for modeling
    feature_columns = [
        'occupancy_scaled', 'is_weekend', 'is_sunday', 'library_open', 'class_hours',
        'activity_period', 'morning_peak', 'afternoon_peak', 'evening_peak',
        'is_holiday', 'is_preliminary', 'study_intensity',
        'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos'
    ]

    # One-Hot Encode categorical features
    encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop first to avoid multicollinearity
    occupancy.dropna(subset=['hour', 'day_of_week'], inplace=True) # Ensure no NaNs in time features
    cat_features = encoder.fit_transform(occupancy[['hour', 'day_of_week']])

    # Get feature names for one-hot encoded features
    cat_feature_names = encoder.get_feature_names_out(['hour', 'day_of_week'])

    # Combine all features
    numerical_features = occupancy[feature_columns].values
    features = np.hstack([numerical_features, cat_features])
    target = occupancy['occupancy_scaled'].values

    # Create feature names for SHAP later
    all_feature_names = feature_columns + list(cat_feature_names)

    print(f"Total features: {len(all_feature_names)}")
    print(f"Feature names: {all_feature_names[:10]}...")  # Show first 10 features

    # Create sequences
    sequence_length = 24  # 24 hours lookback
    X, y = [], []
    for i in range(len(features) - sequence_length):
        X.append(features[i:i+sequence_length])
        y.append(target[i+sequence_length])

    X = np.array(X)
    y = np.array(y)

    if len(X) == 0:
        print(f"Not enough data for {file_name}, skipping...")
        continue

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
    print(f"Training shape: {X_train.shape}")
    print(f"Testing shape: {X_test.shape}")

    # --- 3. Build the Advanced Model (CNN-LSTM with Attention) ---
    n_features = X_train.shape[2]
    input_layer = Input(shape=(sequence_length, n_features))

    # CNN part to extract spatial features from the sequence
    cnn_out = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)
    cnn_out = MaxPooling1D(pool_size=2)(cnn_out)
    cnn_out = Flatten()(cnn_out)

    # LSTM part to learn temporal patterns
    lstm_out = LSTM(units=50, return_sequences=True)(input_layer)

    # Attention mechanism
    # The attention layer computes a score for each LSTM output step
    attention_scores = Dense(1, activation='tanh')(lstm_out)
    attention_scores = Flatten()(attention_scores)
    attention_weights = Dense(sequence_length, activation='softmax')(attention_scores)
    attention_weights = Reshape((sequence_length, 1))(attention_weights)

    # Apply attention weights to LSTM outputs
    context_vector = lstm_out * attention_weights
    context_vector = Flatten()(context_vector)

    # Concatenate all processed features
    combined = concatenate([cnn_out, context_vector])
    dense_out = Dense(100, activation='relu')(combined)  # Increased capacity for more features
    dense_out = Dense(50, activation='relu')(dense_out)
    output_layer = Dense(1)(dense_out)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])

    print(f"\nModel architecture for {file_name}:")
    model.summary()

    # --- 4. Train and Evaluate ---
    print(f"\nTraining model for {file_name}...")
    # NOTE: Setting verbose=0 for final script to reduce output, but kept as 1 for debugging in original snippet
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)

    loss, mae = model.evaluate(X_test, y_test, verbose=0)

    predictions_scaled = model.predict(X_test, verbose=0)
    predictions = occ_scaler.inverse_transform(predictions_scaled)
    y_test_actual = occ_scaler.inverse_transform(y_test.reshape(-1, 1))

    # Flatten arrays for metric calculation (sklearn expects 1D arrays)
    y_true_flat = y_test_actual.flatten()
    y_pred_flat = predictions.flatten()

    # Calculate R2, MSE, RMSE using helper function
    r2, mse, rmse = calculate_metrics(y_true_flat, y_pred_flat)

    print(f"\n--- Evaluation Metrics for {file_name} (Enhanced Model) ---")
    print(f"R-squared (R2): {r2:.4f}")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.2f} (In units of 'Number of Users')")

    # Store results
    results_df.loc[len(results_df)] = [location_name, r2, mse, rmse]

    # --- SAVE MODEL AND SCALER ---
    model_save_path = f'/opt/airflow/models/{location_name}_cnn_only.h5'
    scaler_save_path = f'/opt/airflow/models/{location_name}_cnn_only_scaler.pkl'

    try:
        model.save(model_save_path)
        print(f"Model saved to {model_save_path}")
        import pickle
        with open(scaler_save_path, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"Scaler saved to {scaler_save_path}")
    except Exception as e:
        print(f"Error saving model or scaler for {location_name}: {e}")

    # --- 5. Visualize Predictions ---
    plt.figure(figsize=(15, 8))

    # Main prediction plot
    plt.subplot(2, 1, 1)
    plt.plot(y_test_actual, label='Actual Occupancy', color='blue', alpha=0.7)
    plt.plot(predictions, label='Predicted Occupancy', color='orange', linestyle='--', alpha=0.8)
    plt.title(f'Enhanced Model: Actual vs. Predicted Occupancy for {file_name}')
    plt.xlabel('Time Step')
    plt.ylabel('Number of Users')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Training history
    plt.subplot(2, 1, 2)
    plt.plot(history.history['loss'], label='Training Loss', color='red', alpha=0.7)
    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple', alpha=0.7)
    plt.title('Training History')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{file_name}_enhanced_model_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    # --- 6. SHAP Interpretability ---
    # NOTE: SHAP analysis is kept for completeness but is computationally intensive and might slow down execution.
    print(f"\n--- Running SHAP Analysis for {file_name} ---")

    # Use a smaller sample for SHAP to manage computational complexity
    n_shap_samples = min(30, len(X_test))
    X_train_sample = X_train[:n_shap_samples]
    X_test_sample = X_test[:n_shap_samples]

    # Flatten the sequence for SHAP
    X_train_flat = X_train_sample.reshape(X_train_sample.shape[0], -1)
    X_test_flat = X_test_sample.reshape(X_test_sample.shape[0], -1)

    # Create feature names for plotting
    feature_names = []
    for i in range(sequence_length):
        for name in all_feature_names:
            feature_names.append(f'step_{i}_{name}')

    # Modify the model's predict function to handle the flattened input for SHAP
    def model_predict_flat(x):
        # Reshape the flattened input back to the original sequence shape
        x_reshaped = x.reshape(-1, sequence_length, n_features)
        return model.predict(x_reshaped, verbose=0).flatten()

    try:
        print("Creating SHAP explainer...")
        explainer = shap.KernelExplainer(model_predict_flat, X_train_flat[:10])  # Use fewer background samples

        print("Computing SHAP values...")
        shap_values = explainer.shap_values(X_test_flat[:10], nsamples=50)  # Reduce nsamples for speed

        # Plot SHAP summary
        print("\n--- SHAP Summary Plot ---")
        shap.summary_plot(shap_values, features=X_test_flat[:10], feature_names=feature_names,
                         max_display=20, show=False)  # Show top 20 features
        plt.title(f'SHAP Feature Importance - {file_name}')
        plt.tight_layout()
        plt.savefig(f'{file_name}_shap_summary_enhanced.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Feature importance analysis by time step
        shap_values_array = np.array(shap_values)
        importance_by_feature = np.mean(np.abs(shap_values_array), axis=0)

        # Group importance by base feature (across all time steps)
        feature_importance_grouped = {}
        for i, fname in enumerate(feature_names):
            base_feature = fname.split('_', 2)[-1]  # Remove step prefix
            if base_feature not in feature_importance_grouped:
                feature_importance_grouped[base_feature] = 0
            feature_importance_grouped[base_feature] += importance_by_feature[i]

        # Plot grouped feature importance
        plt.figure(figsize=(12, 8))
        sorted_features = sorted(feature_importance_grouped.items(), key=lambda x: x[1], reverse=True)
        features_to_show = min(15, len(sorted_features))

        feature_names_plot = [item[0] for item in sorted_features[:features_to_show]]
        importance_values = [item[1] for item in sorted_features[:features_to_show]]

        plt.barh(range(len(feature_names_plot)), importance_values)
        plt.yticks(range(len(feature_names_plot)), feature_names_plot)
        plt.xlabel('Average SHAP Importance')
        plt.title(f'Feature Importance Summary - {file_name}')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(f'{file_name}_feature_importance_grouped.png', dpi=300, bbox_inches='tight')
        plt.show()
    except Exception as e:
        print(f"Skipping SHAP analysis for {file_name} due to an error: {e}")


    print(f"Completed analysis for {file_name}")
    print("="*50)


print("\n\n=======================================================")
print("FINAL MODEL EVALUATION SUMMARY (Enhanced CNN-LSTM with Attention)")
print("=======================================================")
print(results_df.to_markdown(index=False, floatfmt=(".2f", ".4f", ".2f", ".2f")))

print("\n")
print(f"This completes the final evaluation script. The summary table above provides the R-squared, MSE, and RMSE for your enhanced model across all locations.")